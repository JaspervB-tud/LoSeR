The objective of this document, is to detail how I proceeded to do things.

* DOWNLOADING DATA
Using NCBI datasets (https://www.ncbi.nlm.nih.gov/datasets/docs/v2/reference-docs/command-line/):
- downloading genomes: 
```bash
datasets download genome taxon TAXID --assembly-level complete --assembly-source GenBank --exclude-atypical --filename FILENAME
```
This downloads all "Complete Genome" quality genomes from GenBank (this excludes duplicates that may also appear in RefSeq) while excluding genomes listed as "atypical" (which are usually hidden when browsing NCBI Genomes).
- downloading metadata:
```bash
datasets summary genome taxon TAXID --assembly-level complete --assembly-source GenBank --exclude-atypical --as-json-lines | dataformat tsv genome --fields accession,organism-tax-id,source_database,assminfo-atypicalis-atypical > metadata.tsv
```
This downloads the metadata (primarily the taxid) for the genomes downloaded through the previous command and stores it in a tsv file called `metadata.tsv`.

* LOCAL SEARCH
For the local search, the general strategy for now is to start from a centroid solution (i.e. select a centroid for every taxon), and to perform local search until an iteration limit or local optimal point has been reached. With this starting point (and in general) it is important to note that in order to remain feasible, we should also consider swaps (i.e. removing one point and adding another) which increases the neighborhoods.

* TO-DO
- Implement local search
    + add a point to the solution (done)
    + remove a point and replace with two points form same cluster (done)
    + swap pair of points within cluster
    + swap pair of points of different clusters
        - note that here we should make sure that we remain feasible
    + remove a point
        - in general this will often lead to a better solution if it is a feasible move
- Optimization
    + now we recalculate the objective value from scratch when checking the neighborhood, but this can be sped up by only considering the terms that change
- Utility
    + initialization is now done by finding centroids for every cluster, but this can be swapped for:
        - random initialization (must be feasible)
        - selecting all points
        - linearization of FLP
    + multi-start option with multiple cores in order to prevent "bad" local optima
- Low-priority
    + visualization of the local search (how does the solution change over time)


* IDEAS
- For the local search we currently only move if we find a better solution. However, there might be merit in considering multiple points in the neighborhood, and moving towards one based on softmax probabilities. Doing so enables searching larger neighborhoods at the cost of potentially obtaining a worse solution. This can be solved by storing the best found solution so far
- Currently implementation requires access to full distance matrix, which is of size O(N^2) with N=#genomes. Ideally, this can be circumvented, for example by only storing "local" distance matrices in memory (within cluster), and re-calculating distances between points of different clusters on a "per-need" basis. Alternatively, this might be able with memory mapping?
- Assigning a constant cost of 1 for picking a genome might be arbitrary. It would be better to find a way of setting this cost to a value that makes sense (e.g. setting it to 0.5 because then it adds a genome if the distance between a pair of genomes is more than 0.5)